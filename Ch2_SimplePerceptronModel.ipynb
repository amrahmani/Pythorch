{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNs6H22gs6kNy4cYh/kgM1+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrahmani/Pythorch/blob/main/SimplePerceptronModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "This code defines a simple Perceptron model with a linear layer and a Sigmoid activation function.\n",
        "It demonstrates training the model on sample data with a Binary Cross Entropy loss function and a Stochastic Gradient Descent (SGD) optimizer.\n",
        "The training loop iterates for a specific number of epochs, calculating the loss, performing backpropagation, and updating the model weights.\n",
        "Finally, the model is tested on new data to predict the class (0 or 1).\n",
        "\n",
        "Further Exploration:\n",
        "\n",
        "Students can experiment with different learning rates, number of epochs, and activation functions (e.g., ReLU).\n",
        "They can modify the code to work with more complex datasets and multiple output classes.\n",
        "This example serves as a foundation for building more advanced neural network architectures in PyTorch."
      ],
      "metadata": {
        "id": "l_gxhKovR1v1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building a Perceptron for Binary Classification**\n",
        "\n",
        "This example demonstrates building a simple Perceptron, a single-layer neural network, for classifying data points as either 0 or 1.\n",
        "\n",
        "1. Import Libraries:"
      ],
      "metadata": {
        "id": "T1Gw_I_tSOBN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpQ-viMNRxO6",
        "outputId": "69af5e00-38d4-4bb8-b8b5-2953feb0b2e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79a28c341870>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Optional: Set random seed for reproducibility\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the Perceptron Model:"
      ],
      "metadata": {
        "id": "kzHLGWVWShRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(Perceptron, self).__init__()\n",
        "    # Define a linear layer with input_dim features and 1 output neuron\n",
        "    self.linear = nn.Linear(input_dim, output_dim)\n",
        "    # Define activation function (e.g., Sigmoid for binary classification)\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Pass the input through the linear layer\n",
        "    z = self.linear(x)\n",
        "    # Apply activation function\n",
        "    a = self.activation(z)\n",
        "    return a"
      ],
      "metadata": {
        "id": "VJz9TljTSmGu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create Sample Data:"
      ],
      "metadata": {
        "id": "uYcNBfjzStf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some sample input data (2 features) and labels (0 or 1)\n",
        "inputs = torch.tensor([[0.7, 0.2], [0.4, 0.5], [0.1, 0.8], [0.9, 0.1]])\n",
        "labels = torch.tensor([1, 0, 0, 1])"
      ],
      "metadata": {
        "id": "EwMDn_kdSufl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Instantiate the Model and Loss Function:"
      ],
      "metadata": {
        "id": "gYZiCMSRS3Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the Perceptron model\n",
        "model = Perceptron(2, 1)  # 2 input features, 1 output neuron\n",
        "\n",
        "# Define loss function (e.g., Binary Cross Entropy for binary classification)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Define optimizer for learning rate updates (e.g., SGD)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  # Learning rate of 0.1"
      ],
      "metadata": {
        "id": "6MeQUBWAS0nV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Train the Model:"
      ],
      "metadata": {
        "id": "wKprPAF5S9l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop (adjust number of epochs for better accuracy)\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  # Forward pass\n",
        "  y_predicted = model(inputs)\n",
        "  # Calculate loss\n",
        "  loss = criterion(y_predicted, labels.float().unsqueeze(1))  # Add a new dimension for column\n",
        "\n",
        "  # Backward pass and update weights\n",
        "  optimizer.zero_grad()  # Reset gradients\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # Print training information (optional)\n",
        "  if epoch % 10 == 0:  # Print every 10 epochs\n",
        "    print(f'Epoch {epoch+1} loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pG7m2UuTA2d",
        "outputId": "68d9c58c-5cff-4b2e-cc75-402047b80683"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 loss: 0.6094\n",
            "Epoch 11 loss: 0.5822\n",
            "Epoch 21 loss: 0.5576\n",
            "Epoch 31 loss: 0.5350\n",
            "Epoch 41 loss: 0.5143\n",
            "Epoch 51 loss: 0.4952\n",
            "Epoch 61 loss: 0.4775\n",
            "Epoch 71 loss: 0.4610\n",
            "Epoch 81 loss: 0.4457\n",
            "Epoch 91 loss: 0.4314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Test the Model:"
      ],
      "metadata": {
        "id": "zB-IK4WCTY3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model on new data\n",
        "with torch.no_grad():  # Disable gradient calculation during testing\n",
        "  new_data = torch.tensor([[0.5, 0.3]])  # Example data point\n",
        "  predicted = model(new_data)\n",
        "  # Round the output to get a binary classification (0 or 1)\n",
        "  predicted_class = torch.round(predicted).item()\n",
        "  print(f'Predicted class for {new_data}: {predicted_class}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lARhLTVfTYjt",
        "outputId": "618bea21-e1ce-47fe-8cb7-6f31618270e3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class for tensor([[0.5000, 0.3000]]): 1.0\n"
          ]
        }
      ]
    }
  ]
}
