{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNdawT+k7LWeLAoivSr8/pO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrahmani/NN/blob/main/Ch6_AutoencoderFraudDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Using PyTorch, create an autoencoder for credit card fraud detection. Train and test it with the following dataset: https://raw.githubusercontent.com/amrahmani/Pythorch/main/CreditDataset.csv\n",
        "\n",
        "This dataset has 31 columns (features) including 'Time', 'V1', 'V2', ..., 'V28', 'Amount', and 'Class'. All of its input data are numbers (positive and negative numbers) and a label 'Class' indicating fraud (1) or legitimate transaction (0).\n"
      ],
      "metadata": {
        "id": "9FOM8kvi1NFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and preprocess the dataset**"
      ],
      "metadata": {
        "id": "cxUv0D4p6y7y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukapGk4n1MVV"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import requests\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, precision_recall_curve, accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset from URL using requests library\n",
        "url = \"https://raw.githubusercontent.com/amrahmani/Pythorch/main/CreditDataset.csv\"\n",
        "response = requests.get(url)  # Send a GET request to the URL\n",
        "data = pd.read_csv(url)  # Read the CSV data into a pandas DataFrame\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.drop('Class', axis=1)  # Rremove column 'Class', dropping a column (axis=1), a row (axis=0)\n",
        "y = data['Class']  # Labels (only 'Class' column)\n",
        "\n",
        "# Standardize the features using StandardScaler\n",
        "scaler = StandardScaler()  # Create a StandardScaler object\n",
        "X_scaled = scaler.fit_transform(X)  # Fit and transform the features\n",
        "\n",
        "# Convert the standardized features and labels to PyTorch tensors\n",
        "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)  # Convert features to tensor\n",
        "y_tensor = torch.tensor(y.values, dtype=torch.float32)  # Convert labels to tensor\n",
        "\n",
        "# Use the last 20% rows as test data\n",
        "num_test_rows = int(len(data) * 0.2)\n",
        "\n",
        "# Split data using the calculated number of test rows\n",
        "X_test = X_tensor[-num_test_rows:]\n",
        "y_test = y_tensor[-num_test_rows:]\n",
        "\n",
        "# Use the remaining data as training data\n",
        "X_train = X_tensor[:-num_test_rows]\n",
        "# y_train = y_tensor[:-num_test_rows] Why we do not need y_train?\n",
        "\n",
        "# Convert training data to DataLoader for PyTorch\n",
        "train_data = torch.utils.data.TensorDataset(X_train, X_train)  # Create a TensorDataset for training data, X_train = y_train why?\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)  # Create a DataLoader for training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define and train the autoencoder**"
      ],
      "metadata": {
        "id": "_AbsbljF7S35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the autoencoder architecture using PyTorch's nn.Module\n",
        "class FraudAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FraudAutoencoder, self).__init__()\n",
        "        # Encoder layers Number of inputs = Number of outputs = 30\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(30, 20),  # Input size: 30, Output size: 20\n",
        "            nn.ReLU(True),  # ReLU activation function\n",
        "            nn.Linear(20, 10),  # Input size: 20, Output size: 10\n",
        "            nn.ReLU(True))  # ReLU activation function\n",
        "        # Decoder layers\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(10, 20),  # Input size: 10, Output size: 20\n",
        "            nn.ReLU(True),  # ReLU activation function\n",
        "            nn.Linear(20, 30),  # Input size: 20, Output size: 30\n",
        "            nn.Tanh())  # Tanh activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)  # Pass input through encoder layers\n",
        "        x = self.decoder(x)  # Pass output through decoder layers\n",
        "        return x\n",
        "\n",
        "# Instantiate the autoencoder model, define loss function and optimizer\n",
        "model = FraudAutoencoder()  # Create an instance of the FraudAutoencoder class\n",
        "criterion = nn.MSELoss()  # Mean Squared Error loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate 0.001\n",
        "\n",
        "# Train the autoencoder\n",
        "num_epochs = 20  # Number of epochs for training\n",
        "for epoch in range(num_epochs):\n",
        "    for data in train_loader:\n",
        "        inputs, _ = data  # Get input data (ignoring labels)\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "\n",
        "        loss = criterion(outputs, inputs)  # Calculate loss (error)\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "    if (epoch+1) % 10 == 0:\n",
        "      print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))  # Print epoch and loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaQWz2Bi7R5J",
        "outputId": "f36628e1-e81f-4179-cc65-cf38bb05a6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/20], Loss: 0.4345\n",
            "Epoch [20/20], Loss: 0.6813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the autoencoder using accuracy, recall, precision, and F-measure**"
      ],
      "metadata": {
        "id": "L7XevBbA7RZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the autoencoder using AUPRC\n",
        "with torch.no_grad():\n",
        "  # reconstruction_error is very important in Autoencoders, it is the difference between the original test data (input) and the data reconstructed by the model.\n",
        "  reconstruction_error = torch.mean((X_test - model(X_test)) ** 2, dim=1) # It calculates the mean squared error between the original test data and the predictions, in Autoencoders inputs = outputs\n",
        "  precision, recall, _ = precision_recall_curve(y_test, reconstruction_error)\n",
        "\n",
        "# Convert reconstruction error to binary predictions (fraud or not fraud) using a threshold of mean()+std()\n",
        "predictions = (reconstruction_error > reconstruction_error.mean()+reconstruction_error.std()).float()\n",
        "\n",
        "# Calculate accuracy, recall, precision, and F-measure\n",
        "accuracy = accuracy_score(y_test, predictions)  # Import from sklearn.metrics\n",
        "recall = recall_score(y_test, predictions)  # Import from sklearn.metrics\n",
        "precision = precision_score(y_test, predictions)  # Import from sklearn.metrics\n",
        "f1 = f1_score(y_test, predictions)  # Import from sklearn.metrics\n",
        "\n",
        "print('Accuracy: {:.4f}'.format(accuracy))\n",
        "print('Recall: {:.4f}'.format(recall))\n",
        "print('Precision: {:.4f}'.format(precision))\n",
        "print('F-measure: {:.4f}'.format(f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8Dppy_o7RCx",
        "outputId": "b85d6549-4bd0-4754-8f48-12570c1fbaae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9858\n",
            "Recall: 0.8444\n",
            "Precision: 0.2197\n",
            "F-measure: 0.3486\n"
          ]
        }
      ]
    }
  ]
}