{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrahmani/Pythorch/blob/main/NeuronPerceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a simple Python code using PyTorch to create a single neuron (***perceptron***) with Sign activation function *italicised text*"
      ],
      "metadata": {
        "id": "dV2kUF_un012"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZBRDcxJj4Ns",
        "outputId": "f349f86a-46c9-4e5e-cfe1-4e10ea981370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: -1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Define input features\n",
        "inputs = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "# Define weights and bias\n",
        "weights = torch.tensor([-0.5, -0.3, 0.1])\n",
        "bias = torch.tensor(0.2)\n",
        "\n",
        "# Define activation function (in this case, a simple step function)\n",
        "def activation(x):\n",
        "    return 1 if x >= 0 else -1\n",
        "\n",
        "# Calculate the weighted sum of inputs and add bias\n",
        "weighted_sum = torch.sum(inputs * weights) + bias\n",
        "\n",
        "# Apply activation function\n",
        "output = activation(weighted_sum)\n",
        "\n",
        "print(\"Output:\", output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a Python code using PyTorch to create a simple Multilayer Perceptron (MLP) with 2 input neurons, one hidden layer with 3 neurons, and one output *neuro*"
      ],
      "metadata": {
        "id": "-_dxwf_Jq4yX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the MLP class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden = nn.Linear(2, 3)  # Hidden layer with 2 input neurons and 3 output neurons\n",
        "        self.output = nn.Linear(3, 1)  # Output layer with 3 input neurons and 1 output neuron\n",
        "        self.activation = nn.ReLU()    # ReLU activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.hidden(x))  # Forward pass through the hidden layer with ReLU activation\n",
        "        x = self.output(x)                   # Forward pass through the output layer\n",
        "        return x\n",
        "\n",
        "# Define input tensor\n",
        "inputs = torch.tensor([[1.0, 2.0]])\n",
        "\n",
        "# Create an instance of the MLP\n",
        "mlp = MLP()\n",
        "\n",
        "# Perform a forward pass\n",
        "output = mlp(inputs)\n",
        "\n",
        "print(\"Output:\", output)\n"
      ],
      "metadata": {
        "id": "eouDGgFW1jLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' These lines import the necessary modules from the PyTorch library.\n",
        "PyTorch is a popular deep learning framework that provides tools for building and training neural networks.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "'''This block of code defines a class called MLP that represents our Multilayer Perceptron model.\n",
        "It inherits from nn.Module, which is the base class for all neural network modules in PyTorch.\n",
        "In Python, super() is a built-in function used to call methods of a superclass (or parent class) in a derived class (or subclass).\n",
        "In the __init__ method, we define the layers of our MLP. nn.Linear represents a fully connected layer,\n",
        "where the first parameter is the number of input neurons and the second parameter is the number of output neurons.\n",
        "So, self.hidden represents the hidden layer with 2 input neurons and 3 output neurons,\n",
        "and self.output represents the output layer with 3 input neurons and 1 output neuron.\n",
        "We also initialize the ReLU activation function (nn.ReLU()) and store it in self.activation.\n",
        "ReLU (Rectified Linear Unit) is a commonly used activation function in neural networks.'''\n",
        "\n",
        "'''forward method defines the forward pass of our MLP. It takes an input tensor x and applies the layers sequentially.\n",
        "x = self.activation(self.hidden(x)) performs a forward pass through the hidden layer (self.hidden) followed\n",
        " by the ReLU activation function (self.activation).\n",
        "x = self.output(x) performs a forward pass through the output layer (self.output), which produces the final output of the model.\n",
        "The output tensor x is returned as the result of the forward pass.'''\n",
        "# Define the MLP class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden = nn.Linear(2, 3)  # Hidden layer with 2 input neurons and 3 output neurons\n",
        "        self.output = nn.Linear(3, 1)  # Output layer with 3 input neurons and 1 output neuron\n",
        "        self.activation = nn.ReLU()    # ReLU activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.hidden(x))  # Forward pass through the hidden layer with ReLU activation\n",
        "        x = self.output(x)                   # Forward pass through the output layer\n",
        "        return x\n",
        "\n",
        "'''This line defines the input data for our MLP as a tensor. In this example, we have a single input data point with two features (2 inputs).'''\n",
        "# Define input tensor\n",
        "inputs = torch.tensor([[1.0, 2.0]])\n",
        "\n",
        "# Create an instance of the MLP\n",
        "mlp = MLP()\n",
        "\n",
        "'''This line performs a forward pass through our MLP model (mlp) with the input data inputs. It calculates the output of the model based on the input data and the weights of the layers.\n",
        "The result is stored in the variable output, '''\n",
        "# Perform a forward pass\n",
        "output = mlp(inputs)\n",
        "\n",
        "print(\"Output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA7Yw45Lq7zB",
        "outputId": "3c8ac672-e8c0-4374-d405-02039ace0264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[-0.4297]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "an MLP with PyTorch with 2 input neurons, 2 hidden layers (with 3 and 4 neurons respectively), and 2 output neurons"
      ],
      "metadata": {
        "id": "iUGHWpNj42pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the MLP class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden1 = nn.Linear(2, 3)  # First hidden layer with 2 input neurons and 3 output neurons\n",
        "        self.hidden2 = nn.Linear(3, 4)  # Second hidden layer with 3 input neurons and 4 output neurons\n",
        "        self.output = nn.Linear(4, 2)   # Output layer with 4 input neurons and 2 output neurons\n",
        "        self.activation = nn.ReLU()     # ReLU activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.hidden1(x))  # Forward pass through the first hidden layer with ReLU activation\n",
        "        x = self.activation(self.hidden2(x))  # Forward pass through the second hidden layer with ReLU activation\n",
        "        x = self.output(x)                    # Forward pass through the output layer\n",
        "        return x\n",
        "\n",
        "# Define input tensor\n",
        "inputs = torch.tensor([[1.0, 2.0]])\n",
        "\n",
        "# Create an instance of the MLP\n",
        "mlp = MLP()\n",
        "\n",
        "# Perform a forward pass\n",
        "output = mlp(inputs)\n",
        "\n",
        "print(\"Output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aKChTVo43WE",
        "outputId": "82666a76-c587-4b5f-d313-334c9ae3638d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[-0.3725, -0.0853]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python code using PyTorch tensors to create an MLP with 2 inputs, 2 hidden layers (the first with 3 neurons and sigmoid activation, the second with 4 neurons and ReLU activation), and 2 outputs with sigmoid activation."
      ],
      "metadata": {
        "id": "qr8OurZU6iO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the MLP class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden1 = nn.Linear(2, 3)  # First hidden layer with 2 input neurons and 3 output neurons\n",
        "        self.hidden2 = nn.Linear(3, 4)  # Second hidden layer with 3 input neurons and 4 output neurons\n",
        "        self.output = nn.Linear(4, 2)   # Output layer with 4 input neurons and 2 output neurons\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.hidden1(x))  # Forward pass through the first hidden layer with sigmoid activation\n",
        "        x = torch.relu(self.hidden2(x))         # Forward pass through the second hidden layer with ReLU activation\n",
        "        x = torch.sign(self.output(x))      # Forward pass through the output layer with sign activation\n",
        "        return x\n",
        "\n",
        "# Define input tensor\n",
        "inputs = torch.tensor([[1.0, 2.0]])\n",
        "\n",
        "# Create an instance of the MLP\n",
        "mlp = MLP()\n",
        "\n",
        "# Perform a forward pass\n",
        "output = mlp(inputs)\n",
        "\n",
        "print(\"Output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbR3SogL6jEx",
        "outputId": "70dc9079-9fbf-4bdf-bb11-96800a164ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[-1.,  1.]], grad_fn=<SignBackward0>)\n"
          ]
        }
      ]
    }
  ]
}
