The emergence of the Internet of Things (IoT) has enabled the proliferation of interconnected devices and sensors, generating vast amounts of often complex and unstructured data. Deep learning (DL), a subfield of machine learning (ML), has shown great promise in addressing the challenges of processing and analyzing such data. Considering the increasing importance of DL and data analysis, we decided to review the articles of the last few years in this field to pave the way for researchers. In this article, we used the systematic literature review (SLR) method, and in line with that, we selected and analyzed 56 articles published from 2019 to April 2024. We first discuss the DL models used in the IoT field and clarify their specific use cases. Secondly, we outline an analysis of research areas in DL-based IoT. In addition, our research extends to the tools and simulators used to evaluate studies in the DL-based IoT domain. We also examine the DL-based IoT research datasets. Finally, our review identifies future directions and open issues in DL-based IoT. We aim to contribute to an accurate understanding of the current state, challenges, and potential breakthroughs at the intersection of DL and the IoT.

The world is changing with the development of the Internet of Things (IoT) and artificial intelligence (AI), particularly their combination. Thanks to the IoT, vast amounts of data are gathered from various sources. However, processing and analyzing the data generated by the numerous IoT devices takes a lot of work. Investing in new technologies is necessary to achieve the objectives and maximize the potential of IoT devices [1, 2].
The convergence of AI and the IoT may change the industry, business, and economic operations. AI uses the IoT to build intelligent machines that mimic intelligent behavior and assist in making decisions with little to no human involvement. Both professionals and laypeople can benefit from the combination of the two. IoT deals with the interaction of devices over the Internet, whereas AI enables devices to learn from their data and experiences. This article explains why combining IoT and AI is necessary [3, 4]. Deep learning (DL) is an essential and rapidly growing field of AI that can change our lives. DL can provide the best output to humans by examining the large and large data available on the Internet and help them a lot in life [5].
DL's significance stems from its capacity to resolve intricate problems beyond machines' capability in the past. Machine learning (ML) algorithms typically acquire the ability to react and behave in the situations presented to them. The significance of DL also lies in its scalability to handle huge datasets or big data. Since there is a lot of data in our daily lives mixed with technology, DL algorithms can examine and process a large amount of data and are considered a practical solution for all industries. DL also has the potential to revolutionize the way humans interact with technology. DL is also used in self-driving cars in most parts of the world [2].
Adding additional technologies, like ML, to an intelligent system boosts its efficiency and production. We can comprehend and enhance the data gathered from physical devices thanks to ML, DL, and AI because IoT devices aim to collect and use data. To provide greater value to IoT, expert systems are employed to analyze data collected from connected devices. Software with machine intelligence features analyzes raw data collected and aggregated by a linked device network. After careful examination, the finished product includes insightful data[2, 5, 6].
Thankfully, improvements in ML paradigms have made it possible to use data analytics in IoT applications. These services are the foundation for IoT applications because DL models have demonstrated significant results in various fields, including image recognition, information retrieval, speech recognition, natural language processing, indoor localization, physiological and psychological state recognition, etc. [4, 7]. 
AI depends on the IoT for its future and worth. AI has shown a notable surge in recent years due to DL. To operate effectively, DL needs a lot of data. Numerous IoT nodes gather this quantity of data from the surrounding environment, and we see the daily growth of these nets and nodes. As a result, the IoT enhances AI's performance. Improved AI performance will also help show off the IoTs' potential and gain widespread acceptance. Both technologies will profit from this cycle in this way. IoT is, therefore, made more effective and beneficial by the application of AI.
Considering the daily progress of AI, especially DL, and also to keep the information updated to make it easier and smoother for researchers, we decided to write this systematic literature review (SLR) article.  Some review studies done in the DL-based IoT field, for instance,  Bhattacharya et al. [3], focused on the state-of-the-art applications of DL in smart cities. The authors surveyed various DL applications in smart city data, providing suggestions for future research areas. However, the review lacked discussion on future challenges and did not propose a taxonomy related to their work. Lakshman et al. [4] focused on the basics of IoT, data generation and processing, and DL techniques in the IoT context. The authors explored IoT fundamentals, DL techniques, and key reporting initiatives for DL in the IoT domain. They covered advantages, uses, and challenges associated with DL's application enabling IoT applications. 
Bolhasani et al. [2] focused on DL in IoT applications within healthcare systems. The authors presented theoretical notions and technical taxonomy related to DL. They analyzed existing research to highlight key DL applications in healthcare and medical sciences. The study provides a comprehensive overview of DL applications in healthcare IoT, offering insights into theoretical frameworks, practical benefits, and areas for future investigation.
Previous survey papers focused solely on a single IoT application, such as smart cities or healthcare, neglecting other aspects. Alternatively, some surveys had shortcomings, which our SLR addresses. To mention previous papers' weaknesses, we can point out that some studies do not provide taxonomy [3-6]. In some other reviews, the review type needs to be clarified [5, 7]. In others, future work and open issues are not considered [3]. Some others still need to provide a comprehensive and sufficient review [3]. In this article, we tried to cover these shortcomings and provide a comprehensive and complete review so that researchers can quickly find an overview in this field and save time. The main contributions of our work are as follows:
	Presenting a new taxonomy in the field of DL-based IoT
	Outlining key areas where future research could improve the use of the DL technique in IoT 
	Exploring tools and simulators for evaluation of DL-based IoT research results
	Discussing common evaluation criteria used in the DL-based IoT domain
	Exploring challenges and future work methods in the field of DL-based IoT
The remainder of this paper is organized as follows: Section 2 discusses some related concepts. Section 3 provides some related work. The methodology presented in Section 4 also illustrates the taxonomy. Section 5 offers the organization and categorization of research articles. Section 6 includes research questions, answers, and results. Finally, Section 7 concludes this article.

This section provides a basic understanding of the key concepts and terms necessary to understand DL-based IoT better. Also, to get familiar with these concepts and words used in the rest of the article.

Like how the human brain and body interact, AI and IoT are closely linked. The human body gathers sensory information from touch, hearing, and sight. The human brain processes this information, transforming light into recognized objects and sounds into speech that can be understood. Following a choice, the human brain communicates with the body to direct actions like selecting a thing or talking. The IoT's network of interconnected sensors provides raw data on what is happening in the outside world, much like the human body. AI is like a human brain; it understands data and decides what to do with it. AI is used in the IoT to understand the environment and make decisions. AI is divided into sub-branches such as ML, DL, etc. DL is also a new branch of the artificial neural network(ANN) [3, 8, 9].
The IoT contains many highly complicated data, such as various sensors, radio frequency records, video and image data, descriptive data, location information, environmental parameters, and sensor network data. These factors pose significant IoT management, analytical, and data mining issues. As a result, there is a critical need for data analysis systems to investigate and evaluate the broad and continuous flow of real-world data applications, including monitoring of temperature, air pollution tracking, stock market, and information security, among others. IoT uses AI to comprehend its surroundings and make decisions. ML, DL, and other sub-branches of AI are separated. Regarding ANN, DL is a new topic [3, 10].
AI and the IoT are pushing the boundaries of data processing and smart business, and this trend will continue in the coming years. The convergence of AI and the IoT has created enormous business potential worldwide. IoT sensors detect external information and replace it with a signal that humans and machines can distinguish from each other. Then, it is time for AI to help build smart machines that support this data to support the decision-making process with minimal or no human intervention [2, 4, 7].
DL is a branch of ML related to extensive layers of neural networks. A deep neural network (DNN) analyzes data in the way a person approaches a problem. DL is widely associated with ML and AI. Of course, this may increase the possibility of misunderstanding and misunderstanding. In other words, DL is an AI function that replicates how the human brain processes data, creates patterns, and applies those patterns to decision-making. In other terms, DL is a sub-branch of ML that processes auditory and visual sensory inputs using many layers of linear transformations [7, 11].
By breaking down each complex notion into simpler ones using this strategy, the computer eventually reaches basic concepts for which it can make judgments, negating the need for constant human oversight to provide the machine with the knowledge it needs. How information is delivered is a crucial DL concern. The machine should be given information in a way that ensures it obtains the crucial details it needs to make decisions in the shortest amount of time possible [3].
Due to Figure 1, everyone is very excited about data, IoT, and AI. IoT refers to numerous things, including objects and devices in our environment connected to the Internet, and can be managed and controlled by applications on smartphones and tablets. In simple terms, the IoT connects sensors and devices with a network through which they can interact with each other and their users. This concept can be as simple as connecting a smartphone to a TV or as complex as monitoring urban infrastructure and traffic. This network includes many devices around us, from washing machines and refrigerators to our clothes [12]. 
In today's technology, ML means eliminating human intervention wherever possible. It means the data can find its patterns and make autonomous decisions without someone writing new code. Every time IoT sensors receive data, someone needs to categorize it, analyze it, and ensure it feeds back to the device for decision-making. However, managing the considerable data flow will not be accessible if we have many data. For example, self-driving cars must make multiple decisions at the moment, and it is entirely impossible to rely on humans. This is where the role of ML appears in the IoT. The IoT's huge and real potential emerges when combined with AI. ML is used in almost all industrial IoT platforms today. Soon, it will probably not be easy to find an IoT-based device that does not use AI [3, 8]. 
DL is a subset of ML distinguished from ML by executing learning independently of human supervision. In ML, a human must typically label the data for the computer to understand it. In contrast, DL algorithms can correctly infer this data without humans' labeling requirements. DL algorithms can determine how several data collection points relate. These algorithms can automatically assign attributes to each data point rather than needing a human to sort through the data and label each. Due to inadequate hardware and processing power, DL has not been functioning. Modern ML algorithms were developed, examined, and improved over time [2].
 
DL has made significant advances in AI. It is also evident that integration of AI with IoT has ushered in a new era of intelligence and connectivity. The combination of DL and IoT has paved the way for transformative applications from smart homes to industrial automation. However, to fully and usefully utilize the potential of AI and IoT integration, we need to delve deeper into the domain of edge computing where large-scale language models (LLM) play a central role [13].
We certainly know that deploying LLM on edge devices will bring both challenges and opportunities. Edge devices such as sensors and smartphones are inherently resource-constrained compared to cloud servers. This limitation creates significant challenges in terms of adapting to the computational needs of LLMs due to their size and complexity. However, advances in hardware, along with optimization techniques, are gradually overcoming these obstacles. Integrating LLMs at the edge brings many benefits and furthers the synergy between DL, IoT and AI [13-15]:
	Low Latency: By processing data locally on edge devices, the latency associated with transferring data to centralized servers for analysis is significantly reduced. This is particularly beneficial for applications that require real-time or near-real-time decision making, such as autonomous vehicles and industrial automation systems.
	Privacy and Security: Edge computing ensures that sensitive data remains local and is processed within the confines of the edge device. This reduces privacy and security concerns associated with moving data to the cloud, especially for applications that deal with personal or confidential information.
	Bandwidth Efficiency: Edge computing reduces the load on network bandwidth by reducing the amount of data that must be transferred to centralized servers. This not only reduces bandwidth usage, but also potentially lowers costs, especially in scenarios with limited or expensive connectivity.
